---
title: "R Notebook"
output:
  html_notebook: default
  html_document: default
---

The purpose of this clustering study is to see how can we group the institution for students. It will not have a lot of clusters, it is just to see a few groups of institution that are close to each other and grouped together. The grouping is based on the attributes that are student related for an application.

```{r}
library(dplyr)

library(readxl)

college <- read_excel("C:/Users/Burcu Kolbay/Downloads/CollegeClusterData.xlsx")
glimpse(college)
```



Latitude and longitudes are not going to be used for this assignment. It could be use to group the close institutions but "city" will be used instead. However, if we plot we see that there is even wrong coordinates in the data:

```{r pressure, echo=FALSE, fig.cap="A caption", out.width = '50%'}
knitr::include_graphics("C:/Users/Burcu Kolbay/Desktop/derbysoft/coordinate_problems_derby.png")
```

```{r}
length(table(college$CITY))
```

The only location related attribute that will be used is "STABBR", which is the state code. "UNITID" will be used because institution names can occur more than one time if one institution has more than one campus.

```{r}
college_data <- college[,-c(2,3,5,6,7)]
head(college_data)
```



```{r}
dim(data.frame(table(college_data$STABBR)))
```

There are 59 state postcode in the data. 


Before missing value imputation, we change the class of categorical values:

```{r}
glimpse(college_data)
```

```{r}
shouldBeCategorical <- c('STABBR', 'LOCALE', 'CONTROL', 'DISTANCEONLY', 'PREDDEG')
for(v in shouldBeCategorical) {
  college_data[[v]] <- as.factor(college_data[[v]])
}
```


In the "IND_INC_PCT_L0", "IND_INC_PCT_M1", "IND_INC_PCT_M2", "IND_INC_PCT_H1" and "IND_INC_PCT_H2" attributes, there is a value named as "PrivacySuppressed". This value is converted into NA. On the other hand, these attributes' class should be numerical.

```{r}
college_data[college_data == "PrivacySuppressed"] <- NA
shouldBeNumerical <- c('IND_INC_PCT_LO', 'IND_INC_PCT_M1', 'IND_INC_PCT_M2', 'IND_INC_PCT_H1','IND_INC_PCT_H2', 'GRAD_DEBT_MDN')
for(v in shouldBeNumerical) {
  college_data[[v]] <- as.numeric(as.character(college_data[[v]]))
}
glimpse(college_data)
```


Checking the missing values:

```{r}
sapply(college_data, function(x) sum(is.na(x)))
```

For missing value imputation, "missMDA" package will be used. Based on dimensionality reduction methods, this packae succesfully imputes datasets with quantitative, categorical and mixed variables. Indeed, it imputes data with principal component methods that take into account the similarities between the observations and the relationships between variables. It has proven to be really competitive in terms of quality of the prediction compared to the state of art methods.

For leave-one-out cross-validation (method.cv="loo"), each cell of the data matrix is alternatively removed and predicted with a FAMD model using ncp.min to ncp.max dimensions. The number of components which leads to the smallest mean square error of prediction (MSEP) is retained. For the Kfold cross-validation (method.cv="Kfold"), pNA percentage of missing values is inserted at random in the data matrix and predicted with a FAMD model using ncp.min to ncp.max dimensions. This process is repeated nbsim times. The number of components which leads to the smallest MSEP is retained. More precisely, for both cross-validation methods, the missing entries are predicted using the imputeFAMD function, it means using it means using the regularized iterative FAMD algorithm (method="Regularized") or the iterative FAMD algorithm (method="EM"). The regularized version is more appropriate to avoid overfitting issues.

#np <- estim_ncpFAMD(college_data)#3- takes too much time to run
```{r}
library(missMDA)
res.impute <- imputeFAMD(college_data, ncp = 3)
```


As default, method is "regularized" in "imputeFAMD" function. This is to avoid the overfitting problems. In the regularized algorithm, the singular values of FAMD are shrinked. 


```{r}
second_part_data <-res.impute$completeObs[,-c(1:6)]
first_part_data <- college_data[,c(1:6)]
college_data <- cbind(first_part_data, second_part_data)
head(college_data)
```

```{r}
college_data$LOCALE[is.na(college_data$LOCALE)]<-21 
college_data$DISTANCEONLY[is.na(college_data$DISTANCEONLY)]<-FALSE 
```

Choosing the attributes we are interested:

```{r}
names(college_data)
```

```{r}
student_data <- college_data[,c(1,2,3,5,6,7,9,10,11)]
head(student_data)
```

At the first thought, one can think that Euclidean distance is a good choice to group observations. However, it is only valid for continuous variables, thus it is not applicable here. Gower distance is a distance metric that can handle mixed data types. For each variable type, a particular distance metric that works well for that type is used and scaled to fall between 0 and 1. Then, a linear combination is calculated to create the final distance matrix. In R, Gower distance can be calculated using "daisy" function. 


```{r}
library(cluster)
disMat<-daisy(student_data[,-1],metric = "gower")
summary(disMat)
gower_mat<-as.matrix(disMat)

```

Output most similar pair:

```{r}
student_data[which(gower_mat==min(gower_mat[gower_mat!=min(gower_mat)]),arr.ind = T)[1,],]
```

```{r}
college[5433:5434,]
```

It is reasonable to have this pair as the most similar one. Because they belong to the same university.
Output most dissimilar pair:

```{r}
student_data[which(gower_mat==max(gower_mat[gower_mat!=max(gower_mat)]),arr.ind = T)[1,],]
```

```{r}
college[c(5483,1739),]
```


The pam algorithm is a clustering approach related to k-means clustering for partitioning a data set into k groups or clusters. In k-medoids clustering, each cluster is represented by one of the data point in the cluster. These points are named cluster medoids. The term medoid refers to an object within a cluster for which average dissimilarity between it and all the other the members of the cluster is minimal. It corresponds to the most centrally located point in the cluster. These objects (one per cluster) can be considered as a representative example of the members of that cluster which may be useful in some situations. Recall that, in k-means clustering, the center of a given cluster is calculated as the mean value of all the data points in the cluster.

K-medoid is a robust alternative to k-means clustering. This means that, the algorithm is less sensitive to noise and outliers, compared to k-means, because it uses medoids as cluster centers instead of means (used in k-means).

The k-medoids algorithm requires the user to specify k, the number of clusters to be generated (like in k-means clustering). A useful approach to determine the optimal number of clusters is the silhouette method. Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a graphical representation of how well each object lies within its cluster.

The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from ???1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.

Calculate silhouette width for many k using PAM:

```{r}
sil_width <- c(NA)
for(i in 2:10){
  pam_fit <- pam(gower_mat, diss=T,k=i)
  sil_width[i]<-pam_fit$silinfo$avg.width
}
```

Plot silhouette width:

```{r}
plot(1:10, sil_width, xlab = "Number of clusters", ylab = "Silhouette width")
lines(1:10, sil_width)
```

Since higher is the better, 3 is chosen as the cluster number.

```{r}
pam_fit <- pam(gower_mat, diss = T, k=3)
pam_results <- student_data %>% dplyr::select(-UNITID) %>% mutate(cluster=pam_fit$clustering) %>% group_by(cluster) %>% do(the_summary=summary(.))
pam_results$the_summary
```


```{r}
student_data[pam_fit$medoids,]
```
 
```{r}
library(ggplot2)
library(Rtsne)
tsne_obj <- Rtsne(gower_mat, is_distance = T)
tsne_data <- tsne_obj$Y %>% data.frame() %>% setNames(c("X","Y")) %>% mutate(cluster=factor(pam_fit$clustering), id=student_data$UNITID) 
ggplot(aes(x=X, y=Y), data=tsne_data)+geom_point(aes(color=cluster))
```

#Future Work

I would like to use latitude and longitude information after they are corrected. It will be usefull to see the institution that are close by spherical distance. It can give an idea to a student to enroll.

In this study, the institutions with the same name but different IDs are not touched. I would like to reduce the dimensionality of the data by having only one line for the institutions with the same name. Average methods could be used for the continous variables. 

In the dataset provided, there is no information about which university is based on which professionalities (medical, law, etc.) If we have this information, the clustering will make more sense since we will have more clusters.
