---
title: "R Notebook"
output: github_document
---

```{r}
library(dplyr)
college <- CollegeClusterData
glimpse(college)
```



Latitude and longitudes are not going to be used for this assignment. It could be use to group the close institutions but "city" will be used instead. However, if we plot we see that there is even wrong coordinates in the data:

```{r pressure, echo=FALSE, fig.cap="A caption", out.width = '50%'}
knitr::include_graphics("C:/Users/Burcu Kolbay/Desktop/derbysoft/coordinate_problems_derby.png")
```

```{r}
length(table(college$CITY))
```

The only location related attribute that will be used is "STABBR", which is the state code. "UNITID" will be used because institution names can occur more than one time if one institution has more than one campus.

```{r}
college <- college[,-c(2,3,5,6,7)]
head(college)
```



```{r}
dim(data.frame(table(college$STABBR)))
```

There are 59 state postcode in the data. 


Before missing value imputation, we change the class of categorical values:

```{r}
glimpse(college)
```

```{r}
shouldBeCategorical <- c('STABBR', 'LOCALE', 'CONTROL', 'DISTANCEONLY', 'PREDDEG')
for(v in shouldBeCategorical) {
  college[[v]] <- as.factor(college[[v]])
}
```


In the "IND_INC_PCT_L0", "IND_INC_PCT_M1", "IND_INC_PCT_M2", "IND_INC_PCT_H1" and "IND_INC_PCT_H2" attributes, there is a value named as "PrivacySuppressed". This value is converted into NA. On the other hand, these attributes' class should be numerical.

```{r}
college[college == "PrivacySuppressed"] <- NA
shouldBeNumerical <- c('IND_INC_PCT_LO', 'IND_INC_PCT_M1', 'IND_INC_PCT_M2', 'IND_INC_PCT_H1','IND_INC_PCT_H2', 'GRAD_DEBT_MDN')
for(v in shouldBeNumerical) {
  college[[v]] <- as.numeric(as.character(college[[v]]))
}
glimpse(college)
```


Checking the missing values:

```{r}
sapply(college, function(x) sum(is.na(x)))
```

For missing value imputation, "missMDA" package will be used. Based on dimensionality reduction methods, this packae succesfully imputes datasets with quantitative, categorical and mixed variables. Indeed, it imputes data with principal component methods that take into account the similarities between the observations and the relationships between variables. It has proven to be really competitive in terms of quality of the prediction compared to the state of art methods.

```{r}
library(missMDA)
np <- estim_ncpFAMD(college)#3
```
For leave-one-out cross-validation (method.cv="loo"), each cell of the data matrix is alternatively removed and predicted with a FAMD model using ncp.min to ncp.max dimensions. The number of components which leads to the smallest mean square error of prediction (MSEP) is retained. For the Kfold cross-validation (method.cv="Kfold"), pNA percentage of missing values is inserted at random in the data matrix and predicted with a FAMD model using ncp.min to ncp.max dimensions. This process is repeated nbsim times. The number of components which leads to the smallest MSEP is retained. More precisely, for both cross-validation methods, the missing entries are predicted using the imputeFAMD function, it means using it means using the regularized iterative FAMD algorithm (method="Regularized") or the iterative FAMD algorithm (method="EM"). The regularized version is more appropriate to avoid overfitting issues.

```{r}
res.impute <- imputeFAMD(college, ncp = 3)
```

As default, method is "regularized" in "imputeFAMD" function. This is to avoid the overfitting problems. In the regularized algorithm, the singular values of FAMD are shrinked. 


```{r}
second_part_data <-res.impute$completeObs[,-c(1:6)]
first_part_data <- college[,c(1:6)]
college_data <- cbind(first_part_data, second_part_data)
head(college_data)
```

```{r}
#college_data$LOCALE[is.na(college_data$LOCALE)]<-21 
#college_data$DISTANCEONLY[is.na(college_data$DISTANCEONLY)]<-FALSE 
```

Choosing the attributes we are interested:

```{r}
names(college_data)
```

```{r}
student_data <- college_data[,c(1,2,3,5,6,7,9,10,11)]
head(student_data)
```

```{r}
disMat<-daisy(student_data[,-1],metric = "gower")
summary(disMat)
gower_mat<-as.matrix(disMat)

```

Output most similar pair:

```{r}
student_data[which(gower_mat==min(gower_mat[gower_mat!=min(gower_mat)]),arr.ind = T)[1,],]
```

Output most dissimilar pair:

```{r}
student_data[which(gower_mat==max(gower_mat[gower_mat!=max(gower_mat)]),arr.ind = T)[1,],]
```

Calculate silhouette width for many k using PAM:

```{r}
sil_width <- c(NA)
for(i in 2:10){
  pam_fit <- pam(gower_mat, diss=T,k=i)
  sil_width[i]<-pam_fit$silinfo$avg.width
}
```

Plot silhouette width:

```{r}
plot(1:10, sil_width, xlab = "Number of clusters", ylab = "Silhouette width")
lines(1:10, sil_width)
```

Since higher is the better, 3 is chosen as the cluster number.

```{r}
pam_fit <- pam(gower_mat, diss = T, k=3)
pam_results <- student_data %>% dplyr::select(-UNITID) %>% mutate(cluster=pam_fit$clustering) %>% group_by(cluster) %>% do(the_summary=summary(.))
pam_results$the_summary
```


```{r}
student_data[pam_fit$medoids,]
```
 
```{r}
library(Rtsne)
tsne_obj <- Rtsne(gower_mat, is_distance = T)
tsne_data <- tsne_obj$Y %>% data.frame() %>% setNames(c("X","Y")) %>% mutate(cluster=factor(pam_fit$clustering), id=student_data$UNITID) 
ggplot(aes(x=X, y=Y), data=tsne_data)+geom_point(aes(color=cluster))
```


